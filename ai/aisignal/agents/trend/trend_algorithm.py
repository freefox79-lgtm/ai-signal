import os
import sys
import numpy as np
from datetime import datetime, timedelta
from typing import List, Dict, Any

# Ensure parent directory is in path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from db_utils import get_db_connection
from data_router import router
from agents.llm.ollama_client import get_ollama_client

class TrendAnalyzer:
    """
    Implements the core algorithmic logic for identifying real-time search trends.
    """
    
    def __init__(self):
        self.ollama = get_ollama_client()
        self.db_url = os.getenv("DATABASE_URL")
        
    def calculate_z_score(self, current_value: float, history: List[float], window_size: int = 24) -> float:
        """
        Calculates a stable Z-Score using a moving average window.
        Z = (X - Î¼_window) / Ïƒ_window
        """
        if not history or len(history) < 3:
            return 0.0
            
        # Use only the last window_size samples for the 24h-like effect
        effective_history = history[-window_size:]
        
        mean = np.mean(effective_history)
        std_dev = np.std(effective_history)
        
        if std_dev < 0.01: # Stabilize against near-zero variance
            return 0.0
            
        z = (current_value - mean) / std_dev
        return round(float(z), 2)
        
    def get_velocity(self, current_value: float, previous_value: float, time_delta_minutes: int) -> float:
        """
        Calculates the rate of change per minute.
        """
        if time_delta_minutes == 0:
            return 0.0
        return (current_value - previous_value) / time_delta_minutes

    # Phase 13: Weighted Scoring Configuration
    WEIGHTS = {
        'search': 0.3,    # Naver/Google
        'sns': 0.25,      # Twitter/Insta
        'community': 0.2, # DC/FMKorea
        'video': 0.15,    # YouTube
        'finance': 0.15   # Stock/Crypto/Market
    }

    def calculate_weighted_score(self, signals: Dict[str, float], category: str = "TECH") -> float:
        """
        Calculates the Total Signal Score with category-specific attenuation (Î»_cat).
        Category Coefficients:
        - FINANCE: 0.5 (Dampen high volatility)
        - TECH/CELEB: 1.0 (Standard)
        - LIFESTYLE/SHOPPING: 1.2 (Boost organic impact)
        """
        total_score = 0.0
        
        # Î»_cat: Dynamic Attenuation Coefficient
        LAMBDA_CAT = {
            'FINANCE': 0.5,
            'TECH': 1.0,
            'CELEB': 1.0,
            'LIFESTYLE': 1.2,
            'SHOPPING': 1.2
        }
        lam = LAMBDA_CAT.get(category.upper(), 1.0)
        
        for source, weight in self.WEIGHTS.items():
            raw_score = signals.get(source, 0.0)
            normalized = min(max(raw_score, 0.0), 100.0)
            total_score += (normalized * weight)
            
        # Apply Category Attenuation
        final_score = total_score * lam
            
        return round(float(final_score), 2)

    def calculate_slope(self, series: List[float]) -> float:
        """
        Calculates the Rate of Change (Slope).
        Formula: (Current - Avg_History) / Avg_History
        """
        if not series or len(series) < 2:
            return 0.0
            
        current = series[-1]
        history = series[:-1]
        avg_hist = np.mean(history)
        
        if avg_hist == 0:
            return 0.0 if current == 0 else 1.0 # Jump from 0 is infinite, cap at 1.0 (100%)
            
        return (current - avg_hist) / avg_hist

    def categorize_keyword(self, item: Dict) -> str:
        """
        Heuristic categorization of keywords.
        """
        keyword = item.get('keyword', '').upper()
        source = item.get('source', '').upper()
        
        # Priority 1: Market/Finance sources
        if any(x in source for x in ['CRYPTO', 'STOCK', 'FINANCE', 'MARKET', 'UPBIT', 'BINANCE']):
            return 'FINANCE'
            
        # Priority 2: Keyword matching
        finance_keywords = ['ì£¼ê°€', 'ë¹„íŠ¸ì½”ì¸', 'ì´ë”ë¦¬ì›€', 'ì‚¼ì„±ì „ì', 'ê¸ˆë¦¬', 'í™˜ìœ¨']
        if any(x in keyword for x in finance_keywords):
            return 'FINANCE'
            
        lifestyle_keywords = ['ì—¬í–‰', 'ìŒì‹', 'ë§›ì§‘', 'íŒ¨ì…˜', 'ì‡¼í•‘', 'í• ì¸', 'íƒ•í›„ë£¨']
        if any(x in keyword for x in lifestyle_keywords):
            return 'LIFESTYLE'
            
        celeb_keywords = ['ì»´ë°±', 'ë£¨ë¨¸', 'ì—´ì• ', 'ì•„ì´ëŒ', 'ë‰´ì§„ìŠ¤', 'BTS']
        if any(x in keyword for x in celeb_keywords):
            return 'CELEB'
            
        return 'TECH' # Default

    def cross_reference_signals(self, candidates: List[Dict]) -> List[Dict]:
        """
        Processes a list of candidates, categorizes them, and calculates the final 
        balanced score using Î»_cat.
        """
        refined_list = []
        
        for item in candidates:
            # 0. Categorization
            cat = self.categorize_keyword(item)
            item['category'] = cat
            
            # 1. Normalize Signals
            z_score = item.get('z_score', 0)
            slope = item.get('slope', 0)
            density = item.get('search_density', 0)
            
            search_raw = (z_score * 20) + (slope * 50) + (density * 0.5)
            
            signals = {
                'search': min(search_raw, 100),
                'video': min(item.get('velocity', 0) * 10, 100),
                'sns': item.get('sns_volume', 0),
                'community': item.get('community_activity', 0),
                'finance': min(item.get('finance_volatility', 0), 100)
            }
            
            # 2. Calculate Weighted Score (with Î»_cat)
            final_score = self.calculate_weighted_score(signals, category=cat)
            item['final_score'] = final_score
            item['signal_breakdown'] = signals
            
            # 3. Determine Status
            if final_score > 80:
                item['status'] = 'BREAKING'
            elif final_score > 50:
                item['status'] = 'VIRAL'
            else:
                item['status'] = 'RISING'
                
            refined_list.append(item)
            
        return sorted(refined_list, key=lambda x: x['final_score'], reverse=True)

    def cluster_keywords(self, candidates: List[Dict]) -> List[Dict]:
        """
        Uses Local LLM to group similar keywords.
        Ex: "Son Heung-min goal", "Sonny highlight" -> "Son Heung-min"
        """
        if not candidates:
            return []
            
        # Prepare list for LLM
        keywords = [c['keyword'] for c in candidates[:20]] # Limit to top 20 for speed
        if not keywords:
            return candidates
            
        prompt = f"""
        ë‹¤ìŒ í‚¤ì›Œë“œë“¤ì„ ê´€ë ¨ëœ ì£¼ì œë³„ë¡œ ê·¸ë£¹í™”í•˜ì„¸ìš”. ê° ê·¸ë£¹ì„ ëŒ€í‘œí•˜ëŠ” í‚¤ì›Œë“œëŠ” ë°˜ë“œì‹œ êµ¬ì²´ì ì´ê³  íŠ¸ë Œë””í•œ **í•œêµ­ì–´** í‚¤ì›Œë“œì—¬ì•¼ í•©ë‹ˆë‹¤.
        
        CRITICAL INSTRUCTION: 
        - Representative Keyword MUST be in **Korean**.
        - It must be specific (e.g., "ì‚¼ì„±ì „ì" instead of "ê¸°ìˆ ì£¼").
        - Do not use generic categories like "ì•”í˜¸í™”í" or "ìŒì‹". Use specific names like "ë¹„íŠ¸ì½”ì¸" or "íƒ•í›„ë£¨".
        
        Keywords: {", ".join(keywords)}
        
        Output Format (JSON):
        [
            {{"representative": "í•œêµ­ì–´í•µì‹¬í‚¤ì›Œë“œ", "members": ["kw1", "kw2"]}},
            ...
        ]
        """
        
        try:
            response = self.ollama.generate(
                prompt=prompt,
                model=self.ollama.MODEL_FAST,
                temperature=0.1,
                max_tokens=500,
                options={
                    "num_ctx": 4096,   # Larger context for list processing
                    "num_gpu": 99      # Force full GPU offload (Mac Mini optimization)
                }
            )
            
            # Parse Mock-ish logic for now if LLM fails or simple rule-based fallback
            # Real implementation needs robust JSON parsing from LLM output
            # For this iteration, we trust the LLM or fallback to identity
            
            # Validating JSON
            import json
            import re
            
            # Extract JSON block if needed
            match = re.search(r'\[.*\]', response, re.DOTALL)
            if match:
                clusters = json.loads(match.group(0))
                
                clustered_results = []
                processed_keywords = set()
                
                for cluster in clusters:
                    rep = cluster['representative']
                    members = cluster['members']
                    
                    # Find the highest scoring member to inherit score
                    best_score = 0
                    best_item = None
                    
                    for m in members:
                        # Find original item
                        orig = next((x for x in candidates if x['keyword'] == m), None)
                        if orig:
                            processed_keywords.add(m)
                            if orig['final_score'] > best_score:
                                best_score = orig['final_score']
                                best_item = orig
                    
                    if best_item:
                        # Create fused item
                        fused = best_item.copy()
                        fused['keyword'] = rep
                        fused['members'] = members
                        clustered_results.append(fused)
                
                # Add leftovers
                for item in candidates:
                    if item['keyword'] not in processed_keywords:
                        clustered_results.append(item)
                        
                return sorted(clustered_results, key=lambda x: x['final_score'], reverse=True)
                
        except Exception as e:
            print(f"âš ï¸ Clustering failed: {e}")
            return candidates # Fallback to original list

        return candidates

    def cross_verify_with_gemma(self, ranked_trends: List[Dict]) -> List[Dict]:
        """
        [Stage 2.7: Deep Reasoning & Governance]
        Uses Gemma 3 12B to cross-verify the analyst's findings and add 'Contrarian' or 'Deep Persona' insights.
        """
        if not ranked_trends:
            return []

        print(f"ğŸ’ [Gemma 3] Performing deep semantic reasoning on top {len(ranked_trends[:5])} trends...")
        
        context = "\n".join([f"- {i['keyword']} (Analytic Score: {i['final_score']})" for i in ranked_trends[:5]])
        
        prompt = f"""
        ë‹¹ì‹ ì€ AI Signal ì‹œìŠ¤í…œì˜ 'ì „ëµì  í•©ì˜ ì—”ì§„' Gemma 3ì…ë‹ˆë‹¤.
        ë¶„ì„ ì „ë¬¸ê°€ê°€ ë„ì¶œí•œ ìƒìœ„ íŠ¸ë Œë“œë¥¼ ê²€í† í•˜ê³ , ê²½ì œì  ê°€ì¹˜ì™€ 'ì‚¬íšŒë¬¸í™”ì  ì˜í–¥ë ¥'ì‚¬ì´ì˜ ê· í˜• ì¡íŒ í†µì°°ì„ ì œì‹œí•˜ì‹­ì‹œì˜¤.
        
        íŠ¸ë Œë“œ ë¦¬ìŠ¤íŠ¸:
        {context}
        
        ì‘ì—…:
        1. ê° íŠ¸ë Œë“œê°€ ëŒ€ì¤‘ì˜ ë¼ì´í”„ìŠ¤íƒ€ì¼ì´ë‚˜ ì‹¬ë¦¬ì— ì–´ë–¤ ë³€í™”ë¥¼ ì¼ìœ¼í‚¬ì§€ ì¶”ë¡ í•˜ì‹­ì‹œì˜¤.
        2. ê¸ˆìœµì  ê´€ì (ì¥„)ê³¼ íŠ¸ë Œë“œ ê´€ì (ì¥í•) ì‚¬ì´ì˜ ì¶©ëŒì„ í•´ê²°í•˜ê³  'ì „ëµì  í•©ì˜(Strategic Consensus)'ë¥¼ ë„ì¶œí•˜ì‹­ì‹œì˜¤.
        3. ë‹¨ìˆœ ìˆ˜ì¹˜ ë¶„ì„ì„ ë„˜ì–´ì„  'ë¬¸í™”ì  ë§¥ë½'ì„ 12B ëª¨ë¸ì˜ ê¹Šì´ë¡œ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.
        
        ì¶œë ¥ í˜•ì‹:
        í•œêµ­ì–´ë¡œ 3-4ë¬¸ì¥ì˜ ê°•ë ¥í•œ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
        """
        
        try:
            # We use the new MODEL_REASONING (Gemma 3)
            reasoning_report = self.ollama.generate(
                prompt=prompt,
                model=self.ollama.MODEL_REASONING,
                temperature=0.4,
                max_tokens=600,
                options={
                    "num_ctx": 8192,
                    "num_gpu": 99 # Maximize Mac Mini GPU power for 12B
                }
            )
            
            # Attach the deep reasoning to the top item or a global field
            if ranked_trends:
                ranked_trends[0]['gemma_deep_reasoning'] = reasoning_report
                
            return ranked_trends
        except Exception as e:
            print(f"âš ï¸ Gemma reasoning failed: {e}")
            return ranked_trends

    def generate_trend_briefing(self, item: Dict) -> str:
        """
        [AI Signal: Strategic Analysis Briefing]
        Direct synthesis focusing on 'Why' it's trending (Search spikes, SNS momentum).
        """
        keyword = item.get('keyword', 'Unknown')
        category = item.get('category', 'TECH')
        score = item.get('final_score', 0)
        z_value = item.get('z_score', 0)
        source = item.get('source', 'System')
        
        prompt = f"""
        ë‹¹ì‹ ì€ AI Signal ì‹œìŠ¤í…œì˜ ë¶„ì„ ì—”ì§„ Gemma 3ì…ë‹ˆë‹¤. 
        ë°ì´í„° ì‹œê·¸ë„ì„ ë¶„ì„í•˜ì—¬ ì´ íŠ¸ë Œë“œê°€ 'ì™œ' ë°œìƒí–ˆëŠ”ì§€ ëŒ€ì¤‘ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì§ê´€ì ì¸ ë¸Œë¦¬í•‘ì„ ì‘ì„±í•˜ì„¸ìš”.
        
        [ë°ì´í„° ìš”ì•½]
        - í‚¤ì›Œë“œ: {keyword} ({category})
        - ë¶„ì„ ì§€í‘œ: ê²€ìƒ‰ëŸ‰(Z-Score {z_value}), ì‚¬íšŒì  í™”ì œì„±(Score {score})
        - ì£¼ìš” ì‹ í˜¸: {source}
        
        [ì‘ì—… ê°€ì´ë“œ]
        1. "ë„¤ì´ë²„ ê²€ìƒ‰ëŸ‰ì´ ì „ì¼ ëŒ€ë¹„ ê¸‰ì¦", "SNS ë° ì»¤ë®¤ë‹ˆí‹° ì–¸ê¸‰ íšŸìˆ˜ ëŒ€í­ ì¦ê°€", "ì£¼ìš” ë§¤ì²´ ë³´ë„" ë“± êµ¬ì²´ì ì¸ ì›ì¸ì„ ì–¸ê¸‰í•˜ì„¸ìš”.
        2. í˜ë¥´ì†Œë‚˜(ì¥„/ì¥í•) ì–¸ê¸‰ ì—†ì´, ê°ê´€ì ì´ê³  ì „ë¬¸ê°€ì ì¸ ì–´ì¡°ë¡œ í•µì‹¬ë§Œ ì „ë‹¬í•˜ì„¸ìš”.
        3. ë°˜ë“œì‹œ 2~3ë¬¸ì¥ì˜ ì§§ê³  ê°•ë ¬í•œ Markdown í…ìŠ¤íŠ¸ë¡œ í•œêµ­ì–´ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.
        
        [ì¶œë ¥ ì–‘ì‹ ì˜ˆì‹œ]
        ### [AI SIGNAL: ì „ëµ ë¶„ì„]
        í•´ë‹¹ í‚¤ì›Œë“œëŠ” í˜„ì¬ ë„¤ì´ë²„ ê²€ìƒ‰ëŸ‰ì´ ê¸‰ê²©íˆ ìƒìŠ¹ ì¤‘ì´ë©°, ì£¼ìš” ì»¤ë®¤ë‹ˆí‹°ì™€ SNSìƒì—ì„œ ì–¸ê¸‰ íšŸìˆ˜ê°€ ë¹ ë¥´ê²Œ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ê´€ë ¨ ë§¤ì²´ì˜ ë³´ë„ê°€ ì´ì–´ì§€ë©° ì‚¬íšŒì  í™”ì œì„±ì´ ì„ê³„ì¹˜ë¥¼ ëŒíŒŒí•œ ê²ƒìœ¼ë¡œ ë¶„ì„ë©ë‹ˆë‹¤. í–¥í›„ 48ì‹œê°„ ë™ì•ˆ ë†’ì€ ëª¨ë©˜í…€ì´ ìœ ì§€ë  ê²ƒìœ¼ë¡œ ì˜ˆì¸¡ë©ë‹ˆë‹¤.
        """
        
        try:
            response = self.ollama.generate(
                prompt=prompt,
                model=self.ollama.MODEL_REASONING, # Using Gemma 3 12B
                temperature=0.3,
                max_tokens=400,
                options={
                    "num_ctx": 4096,
                    "num_gpu": 99 # Maximize M4 Metal
                }
            )
            report = response.strip()
            if not report.startswith("###"):
                report = f"### [AI SIGNAL: ì „ëµ ë¶„ì„]\n{report}"
            return report
        except Exception as e:
            print(f"âš ï¸ Briefing gen failed for {keyword}: {e}")
            return f"### [AI SIGNAL: ì „ëµ ë¶„ì„]\n\ní•´ë‹¹ í‚¤ì›Œë“œì˜ ë„¤ì´ë²„ ê²€ìƒ‰ëŸ‰ ë° SNS í™”ì œì„± ì§€í‘œê°€ ì„ê³„ì¹˜ë¥¼ ëŒíŒŒí•˜ì—¬ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤."

    def save_trends_to_db(self, trends: List[Dict]):
        """
        Saves the processed trends to 'active_realtime_trends' using the DataRouter.
        """
        # 1. Clear old active trends
        router.execute_query("DELETE FROM active_realtime_trends", table_hint='active_realtime_trends')
        
        import json
        # 2. Insert new ones
        for i, t in enumerate(trends[:10]): # Top 10
            router.execute_query("""
                INSERT INTO active_realtime_trends 
                (rank, keyword, avg_score, related_insight, status, source, link, signal_breakdown)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            """, (
                i+1, 
                t['keyword'], 
                t['final_score'], 
                t.get('related_insight') or t.get('reason') or 'AI Detection', 
                t.get('status', 'NEW'),
                t.get('source', 'System'),
                t.get('link', '#'),
                json.dumps(t.get('signal_breakdown', {}))
            ), table_hint='active_realtime_trends')
        
        print(f"âœ… [TrendAnalyzer] Saved {min(len(trends), 10)} trends via DataRouter.")
