"""
Ollama ë¡œì»¬ LLM í´ë¼ì´ì–¸íŠ¸

Mac Mini ìµœì í™”:
- ë¡œì»¬ Ollama ì„œë²„ ì‚¬ìš© (GPU ê°€ì†)
- ì„ë² ë”© ìºì‹± (Redis)
- LLM ìƒì„± ìºì‹± (Redis)
- ìºì‹œ íˆíŠ¸ìœ¨ ëª¨ë‹ˆí„°ë§
- ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›
"""

import os
import requests
import json
import hashlib
import redis
from typing import List, Optional, Union, Dict, Any
from dotenv import load_dotenv
# Load environment variables (WITHOUT OVERRIDE to respect Docker/System env)
if os.path.exists(".env.local"):
    load_dotenv(".env.local", override=False)
else:
    load_dotenv(override=False)


# TTL ìƒìˆ˜ import
try:
    from agents.cache.cache_ttl import CacheTTL
except ImportError:
    # Fallback if cache module not available
    class CacheTTL:
        EMBEDDING = 7 * 24 * 3600
        LLM_GENERATION = 3600


class OllamaClient:
    """Ollama ë¡œì»¬ LLM í´ë¼ì´ì–¸íŠ¸"""
    
    # ëª¨ë¸ ìƒìˆ˜ (Mac Mini ìµœì í™”ìš©)
    MODEL_FAST = "llama3.2:3b"       # ê³ ì† ì²˜ë¦¬, ìš”ì•½ìš©
    MODEL_ANALYTIC = "llama3.2:3b"   # (ì„ì‹œ) Qwen ë¶€ì¬ë¡œ Llamaë¡œ ëŒ€ì²´
    MODEL_REASONING = "gemma3:12b"    # ì‹¬ì¸µ ì¶”ë¡ , í˜ë¥´ì†Œë‚˜ í•©ì„±ìš©
    MODEL_BALANCED = "llama3.2:3b"    # ë²”ìš© ëª¨ë¸
    MODEL_EMBED = "nomic-embed-text" # ì„ë² ë”© ì „ìš©
    
    def __init__(
        self, 
        base_url: str = None,
        default_model: str = "llama3.2:3b"
    ):
        # Try multiple environment variables (Priority: OLLAMA_BASE_URL > OLLAMA_HOST)
        env_url = os.getenv("OLLAMA_BASE_URL") or os.getenv("OLLAMA_HOST")
        self.base_url = base_url or env_url or "http://host.docker.internal:11434"
        self.default_model = default_model
        
        # Redis ìºì‹±
        try:
            redis_pwd = os.getenv("REDIS_PASSWORD", "aisignal2026_secure")
            self.redis = redis.Redis(
                host=os.getenv("REDIS_HOST", "aisignal-redis"),
                port=6379,
                password=redis_pwd,
                decode_responses=False
            )
            self.redis.ping()
            print("[Ollama] Redis ìºì‹± í™œì„±í™” (ì„ë² ë”© + ìƒì„±)")
        except Exception as e:
            print(f"[Ollama] Redis ì—°ê²° ì‹¤íŒ¨, ìºì‹± ë¹„í™œì„±í™”: {e}")
            self.redis = None
        
        # ìºì‹œ ëª¨ë‹ˆí„° (ì„ íƒì )
        self.cache_monitor = None
        try:
            from agents.cache.cache_monitor import get_cache_monitor
            self.cache_monitor = get_cache_monitor()
        except ImportError:
            pass
            
        # GPU / Metal Status
        self.gpu_accelerated = self.check_gpu_status()

    def check_gpu_status(self) -> bool:
        """
        Checks if GPU acceleration (Metal) is active for the current model.
        Uses 'ollama ps' command for verification.
        """
        import subprocess
        try:
            result = subprocess.run(['ollama', 'ps'], capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                output = result.stdout.lower()
                # Pattern: Look for 100% GPU or Metal mentions
                is_gpu = "100% gpu" in output or "metal" in output
                if is_gpu:
                    print(f"ğŸš€ [Ollama] M4 Metal Acceleration confirmed active.")
                else:
                    print(f"âš ï¸ [Ollama] GPU acceleration not detected in current status.")
                return is_gpu
        except Exception as e:
            print(f"â„¹ï¸ [Ollama] GPU status check skip (Host access needed): {e}")
        return False
    
    def _get_cache_key(self, text: str, model: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„± (í…ìŠ¤íŠ¸ + ëª¨ë¸ í•´ì‹œ)"""
        content = f"{text}:{model}"
        return f"ollama:embed:{hashlib.md5(content.encode()).hexdigest()}"
    
    def generate(
        self,
        prompt: str,
        model: str = None,
        temperature: float = 0.7,
        max_tokens: int = 500,
        stream: bool = False,
        use_cache: bool = True,
        options: Optional[Dict[str, Any]] = None,
        keep_alive: str = "5m"
    ) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„± (ìºì‹± ì§€ì›)"""
        model = model or self.default_model
        
        # ìºì‹± (ìŠ¤íŠ¸ë¦¬ë° ì œì™¸, temperature < 0.3ë§Œ)
        if use_cache and not stream and temperature < 0.3 and self.redis:
            cache_key = self._get_generation_cache_key(prompt, model, temperature, max_tokens)
            
            try:
                cached = self.redis.get(cache_key)
                if cached:
                    if self.cache_monitor:
                        self.cache_monitor.record_hit('llm_generation')
                    return cached.decode('utf-8')
            except Exception as e:
                print(f"[Ollama] Cache read error: {e}")
        
        url = f"{self.base_url}/api/generate"
        
        # Default options
        final_options = {
            "num_predict": max_tokens,
            "temperature": temperature
        }
        
        # Merge user options (e.g. num_ctx, num_gpu)
        if options:
            final_options.update(options)

        payload = {
            "model": model,
            "prompt": prompt,
            "stream": stream,
            "options": final_options,
            "keep_alive": keep_alive
        }
        
        try:
            # Optional: Log GPU check before heavy generation
            if model == self.MODEL_REASONING:
                self.check_gpu_status()
                
            # Increased timeout for heavy models (12B+)
            request_timeout = 180 if model == self.MODEL_REASONING else 60
            
            try:
                response = requests.post(url, json=payload, timeout=request_timeout)
                response.raise_for_status()
            except requests.exceptions.HTTPError as he:
                # Automatic Fallback for Reasoning Model (Gemma 3 -> Llama 3)
                if model == self.MODEL_REASONING:
                    print(f"âš ï¸ [Ollama] {model} failed (500), falling back to {self.MODEL_FAST}...")
                    payload['model'] = self.MODEL_FAST
                    response = requests.post(url, json=payload, timeout=60)
                    response.raise_for_status()
                else:
                    raise he
            
            if stream:
                return response.iter_lines()
            else:
                result = response.json()
                response_text = result.get("response", "")
                
                # ìºì‹±
                if use_cache and temperature < 0.3 and self.redis:
                    try:
                        self.redis.setex(cache_key, CacheTTL.LLM_GENERATION, response_text)
                        if self.cache_monitor:
                            self.cache_monitor.record_miss('llm_generation')
                    except Exception as e:
                        print(f"[Ollama] Cache write error: {e}")
                
                return response_text
                
        except Exception as e:
            print(f"[Ollama] Generate error: {e}")
            raise
    
    def _get_generation_cache_key(self, prompt: str, model: str, temperature: float, max_tokens: int) -> str:
        """ìƒì„± ìºì‹œ í‚¤ ìƒì„±"""
        cache_data = {
            'prompt': prompt,
            'model': model,
            'temperature': temperature,
            'max_tokens': max_tokens
        }
        content_hash = hashlib.md5(json.dumps(cache_data, sort_keys=True).encode()).hexdigest()
        return f"ollama:gen:{content_hash}"
    
    def embed(self, text: str, model: str = "nomic-embed-text") -> List[float]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© (GraphRAGìš©, ìºì‹± ì§€ì›)"""
        if self.redis:
            cache_key = self._get_cache_key(text, model)
            try:
                cached_embedding = self.redis.get(cache_key)
                if cached_embedding:
                    if self.cache_monitor:
                        self.cache_monitor.record_hit('embedding')
                    return json.loads(cached_embedding.decode('utf-8'))
            except Exception as e:
                print(f"[Ollama] Cache read error: {e}")
        
        try:
            response = requests.post(
                f"{self.base_url}/api/embeddings",
                json={
                    "model": model,
                    "prompt": text
                },
                timeout=30
            )
            response.raise_for_status()
            embedding = response.json()["embedding"]
            
            # ìºì‹± (7ì¼, ì„ë² ë”©ì€ ê²°ì •ë¡ ì )
            if self.redis:
                try:
                    self.redis.setex(
                        self._get_cache_key(text, model),
                        CacheTTL.EMBEDDING,
                        json.dumps(embedding)
                    )
                    if self.cache_monitor:
                        self.cache_monitor.record_miss('embedding')
                except Exception as e:
                    print(f"[Ollama] Cache write error: {e}")
            
            return embedding
        except Exception as e:
            print(f"[Ollama] ì„ë² ë”© ì˜¤ë¥˜: {e}")
            # 768ì°¨ì› ì œë¡œ ë²¡í„° ë°˜í™˜ (fallback)
            return [0.0] * 768
    
    def chat(
        self, 
        messages: List[Dict[str, str]], 
        model: str = None,
        temperature: float = 0.7
    ) -> str:
        """ì±„íŒ… (ëŒ€í™”í˜•)"""
        model = model or self.default_model
        
        try:
            response = requests.post(
                f"{self.base_url}/api/chat",
                json={
                    "model": model,
                    "messages": messages,
                    "stream": False,
                    "options": {
                        "temperature": temperature
                    }
                },
                timeout=60
            )
            response.raise_for_status()
            return response.json()["message"]["content"]
        except Exception as e:
            print(f"[Ollama] ì±„íŒ… ì˜¤ë¥˜: {e}")
            return f"Error: {str(e)}"
    
    def list_models(self) -> List[str]:
        """ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            response.raise_for_status()
            models = response.json().get("models", [])
            return [m["name"] for m in models]
        except Exception as e:
            print(f"[Ollama] ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def is_available(self) -> bool:
        """Ollama ì„œë¹„ìŠ¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
            
    def warmup(self, models: List[str] = None):
        """ëª¨ë¸ ì˜ˆì—´ (ë©”ëª¨ë¦¬ì— ë¡œë“œ)"""
        models = models or [self.default_model, "mistral:7b"]
        print(f"[Ollama] Warming up models: {models}")
        for model in models:
            try:
                # keep_alive: -1 maintains the model in memory indefinitely/long-term
                requests.post(
                    f"{self.base_url}/api/generate",
                    json={"model": model, "keep_alive": -1},
                    timeout=5 # Trigger only
                )
            except Exception as e:
                # We expect a possible timeout if we don't wait for the full load,
                # but the server will continue loading it.
                pass


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_ollama_client = None

def get_ollama_client() -> OllamaClient:
    """Ollama í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤"""
    global _ollama_client
    if _ollama_client is None:
        _ollama_client = OllamaClient()
    return _ollama_client
