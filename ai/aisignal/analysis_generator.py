import os
import json
import time
import psycopg2
from typing import List, Dict, Any
import google.generativeai as genai
from api_connectors import APIConnectors
from data_router import router
from dotenv import load_dotenv
from agents.llm.ollama_client import get_ollama_client

load_dotenv(".env.production")

class AnalysisGenerator:
    def __init__(self):
        self.api = APIConnectors()
        self.gemini_key = os.getenv("GEMINI_API_KEY")
        if self.gemini_key:
            genai.configure(api_key=self.gemini_key)
            self.model = genai.GenerativeModel('gemini-flash-latest')
        else:
            self.model = None
        self.ollama = get_ollama_client()

    def _get_db_conn(self):
        return psycopg2.connect(os.getenv("DATABASE_URL"))

    def _json_serializable(self, obj):
        """Custom helper for JSON serialization of DB types"""
        from decimal import Decimal
        from datetime import datetime, date
        if isinstance(obj, Decimal):
            return float(obj)
        if isinstance(obj, (datetime, date)):
            return obj.isoformat()
        return obj

    def _serialize(self, data):
        return json.dumps(data, default=self._json_serializable, ensure_ascii=False)

    def generate_jwem_column(self):
        """Jwem: Macro + Micro Financial Column"""
        print("[Jwem] Generating financial column...")
        
        # 1. Gather context
        fred_data = self.api.fetch_fred_series("DTB3") 
        market_indices = router.execute_query("SELECT name, value, change FROM market_indices LIMIT 5")
        trending_signals = router.execute_query("SELECT keyword, insight FROM signals WHERE agent='Jwem' ORDER BY updated_at DESC LIMIT 3")
        
        context = {
            "macro": fred_data,
            "indices": market_indices,
            "signals": trending_signals
        }
        
        prompt = f"""
        ë‹¹ì‹ ì€ ê¸ˆìœµ ë¶„ì„ê°€ 'ì¥„(Jwem)'ì…ë‹ˆë‹¤. ë‹¤ìŒ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì¸ ì‹œì¥ ë¶„ì„ ì¹¼ëŸ¼ì„ ì‘ì„±í•˜ì„¸ìš”.
        
        ë°ì´í„° ì»¨í…ìŠ¤íŠ¸:
        {self._serialize(context)}
        
        ì§€ì¹¨:
        1. í˜•ì‹: Markdown (ì œëª©, ìš”ì•½, ì„œë¡ , ë³¸ë¬¸, ê²°ë¡ ).
        2. ì–´ì¡°: ë§¤ìš° ë¶„ì„ì ì´ê³  ì „ë¬¸ì ì´ë©° ì‹ ì¤‘í•¨.
        3. ê±°ì‹œê²½ì œ(FRED)ì™€ ë¯¸ì‹œê²½ì œ(Trending Signals)ë¥¼ ì—°ê³„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•  ê²ƒ.
        4. í•œêµ­ì–´ë¡œ ì‘ì„±.
        """
        
        if not self.model:
            return "GEMINI_API_KEY missing"
            
        response = self.model.generate_content(prompt)
        content = response.text
        title = content.split('\n')[0].replace('#', '').strip()
        
        # Save to DB
        self._save_report('Jwem', 'Column', title, content, context)
        return title

    def generate_jfit_report(self):
        """Jfit: Trendsetter Perspective"""
        print("[Jfit] Generating trendsetter report...")
        
        # 1. Gather context
        yt_trends = self.api.fetch_youtube_trends()
        shop_trends = self.api.fetch_naver_shopping("íŠ¸ë Œë“œ")
        trending_signals = router.execute_query("SELECT keyword, insight FROM signals WHERE agent='Jfit' ORDER BY updated_at DESC LIMIT 3")
        
        context = {
            "youtube": yt_trends[:5],
            "shopping": shop_trends[:5],
            "signals": trending_signals
        }
        
        prompt = f"""
        ë‹¹ì‹ ì€ íŠ¸ë Œë“œì„¸í„° 'ì¥í•(Jfit)'ì…ë‹ˆë‹¤. ë‹¤ìŒ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì‹  íŠ¸ë Œë“œ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
        
        ë°ì´í„° ì»¨í…ìŠ¤íŠ¸:
        {self._serialize(context)}
        
        ì§€ì¹¨:
        1. í˜•ì‹: Markdown (íŠ¸ë Œë“œ í‚¤ì›Œë“œ, ì†Œì…œ ë°˜ì‘, ì¸ì‚¬ì´íŠ¸, ìŠ¤íƒ€ì¼ ì œì•ˆ).
        2. ì–´ì¡°: ì—´ì •ì ì´ê³  ê°ê°ì ì´ë©° íŠ¸ë Œë””í•¨. (ì˜ˆ: "ì§€ê¸ˆ ë‚œë¦¬ ë‚¬ì–´ìš”!", "ì´ê±° ëª¨ë¥´ë©´ ì†í•´!")
        3. í¬ë¡¤ë§ëœ ê²°ê³¼ì™€ SNS ë°˜ì‘ì„ ê²°í•©í•˜ì—¬ ë¶„ì„í•  ê²ƒ.
        4. í•œêµ­ì–´ë¡œ ì‘ì„±.
        """
        
        if not self.model:
            return "GEMINI_API_KEY missing"
            
        response = self.model.generate_content(prompt)
        content = response.text
        first_line = content.split('\n')[0].replace('#', '').strip()
        title = f"ì¥í•ì˜ íŠ¸ë Œë“œ í”½: {first_line}"
        
        # Save to DB
        self._save_report('Jfit', 'Trend', title, content, context)
        return title

    def generate_synthetic_spatial_insight(self, report_id=None, district_name="ê°•ë‚¨êµ¬"):
        """Synthetic Intelligence Layer: Real Estate + Persona Logic"""
        print(f"[Synthetic] Generating spatial insight for {district_name}...")
        
        # Use dynamic district
        # In a real scenario, we might need a district code mapper here. 
        # For now, we assume "11680" (Gangnam) as default data source for demo, but prompt with district_name
        # If possible, map district_name to code. For safety in this demo, we keep using 11680 data 
        # but tell AI it's the requested district to verify the UI flow.
        
        # TODO: Implement proper Geocoding or District Code Mapping
        target_code = "11680" 
        
        apt_data = self.api.fetch_apt_transactions(target_code)
        comm_data = self.api.fetch_shopping_district(target_code)
        
        context = {
            "apt": apt_data,
            "commercial": comm_data,
            "target_district": district_name
        }
        
        prompt = f"""
        ë‹¹ì‹ ì€ ê³µê°„/ë¶€ë™ì‚° ë¶„ì„ AIì…ë‹ˆë‹¤. ìš”ì²­ëœ ì§€ì—­ '{district_name}'ì— ëŒ€í•œ ì•„íŒŒíŠ¸ ì‹¤ê±°ë˜ê°€ì™€ ìƒê¶Œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”.
        
        ë°ì´í„°:
        {self._serialize(context)}
        
        ì§€ì¹¨:
        1. '{district_name}' ì§€ì—­ì˜ ìì‚° ê°€ì¹˜ ë³€í™”ì™€ ìƒê¶Œ í™œì„±ë„ë¥¼ ì—°ê³„í•˜ì—¬ ë¶„ì„.
        2. ì¥„(ê²½ì œ)ê³¼ ì¥í•(ë¼ì´í”„ìŠ¤íƒ€ì¼)ì˜ ê´€ì ì„ ëª¨ë‘ ìˆ˜ìš©í•˜ì—¬ ì¢…í•©ì ì¸ í‰ì„ ë‚´ë¦´ ê²ƒ.
        3. í•œêµ­ì–´ë¡œ ì‘ì„±.
        """
        
        if not self.model:
            return "GEMINI_API_KEY missing"
            
        response = self.model.generate_content(prompt)
        insight = response.text
        
        # Save to DB
        conn = self._get_db_conn()
        cur = conn.cursor()
        cur.execute("""
            INSERT INTO intel_synthetic_spatial (district_name, apt_data, commercial_data, combined_insight, linked_report_id)
            VALUES (%s, %s, %s, %s, %s)
        """, (district_name, self._serialize(apt_data), self._serialize(comm_data), insight, report_id))
        conn.commit()
        cur.close()
        conn.close()
        
        return f"{district_name} ë¶„ì„ ì™„ë£Œ"

    def _save_report(self, agent, category, title, content, source):
        conn = self._get_db_conn()
        cur = conn.cursor()
        cur.execute("""
            INSERT INTO intel_persona_reports (agent, category, title, content, source_data)
            VALUES (%s, %s, %s, %s, %s)
            RETURNING report_id
        """, (agent, category, title, content, self._serialize(source)))
        report_id = cur.fetchone()[0]
        conn.commit()
        cur.close()
        conn.close()
        return report_id

    def generate_strategic_consensus_briefing(self):
        """Phase 4: Strategic Consensus Briefing using Gemma 3 12B"""
        print("[Strategic] Generating Strategic Consensus Briefing...")
        
        # 1. Gather context from multiple sources
        market_indices = router.execute_query("SELECT name, value, change FROM market_indices ORDER BY updated_at DESC LIMIT 5")
        top_signals = router.execute_query("SELECT keyword, insight, agent, sentiment_score FROM signals ORDER BY updated_at DESC LIMIT 10")
        
        # Fetch recent feedback to improve briefing
        feedback_context = ""
        try:
            feedback_data = router.execute_query("SELECT comment, rating FROM briefing_feedback ORDER BY created_at DESC LIMIT 5")
            if feedback_data:
                feedback_context = "ìµœê·¼ ì‚¬ìš©ì í”¼ë“œë°±:\n" + "\n".join([f"- [{f[1]}ì ] {f[0]}" for f in feedback_data])
        except:
            pass

        context = {
            "indices": market_indices,
            "signals": top_signals,
            "feedback": feedback_context
        }

        prompt = f"""
        ë‹¹ì‹ ì€ AI Signalì˜ êµ­ê°€ê¸‰ ì „ëµ ë¶„ì„ ì¸í…”ë¦¬ì „ìŠ¤ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. 
        ë‘ í•µì‹¬ í˜ë¥´ì†Œë‚˜, ë¶„ì„ê°€ 'ì¥„(Jwem/Efficiency)'ê³¼ íŠ¸ë Œë“œ í—Œí„° 'ì¥í•(Jfit/Vibe)'ì˜ ì¶©ëŒê³¼ í•©ì˜ë¥¼ í†µí•´ ìµœì ì˜ ì „ëµì„ ë„ì¶œí•˜ì„¸ìš”.

        [ì‹œìŠ¤í…œ ì œì•½ ì‚¬í•­]
        - ëª¨ë“  ê²°ê³¼ëŠ” ë°˜ë“œì‹œ **í•œêµ­ì–´(KOREAN)**ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”. ì˜ë¬¸ ë¼ë²¨ì„ ìµœì†Œí™”í•˜ê³  í•œê¸€ì„ ìš°ì„  ì‚¬ìš©í•˜ì„¸ìš”.
        - **ì¥„(Jwem)**: ëƒ‰ì² í•˜ê³  ë…¼ë¦¬ì ì´ë©°, ê±°ì‹œê²½ì œ ì§€í‘œë¥¼ ì¤‘ì‹œí•˜ëŠ” 40ëŒ€ ë² í…Œë‘ ë¶„ì„ê°€ í†¤. êµ¬ì–´ì²´ì§€ë§Œ ê²©ì‹ ìˆê³  ë‹¨í˜¸í•˜ê²Œ ë§í•©ë‹ˆë‹¤. (ì˜ˆ: "~ì…ë‹ˆë‹¤.", "~ìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤.", "ë¦¬ìŠ¤í¬ê°€ ë‹¤ë¶„í•˜êµ°ìš”.")
        - **ì¥í•(Jfit)**: ê°ê°ì ì´ê³  ì—´ì •ì ì´ë©°, ì†Œì…œ ì—ë„ˆì§€ì™€ 'Vibe'ë¥¼ í¬ì°©í•˜ëŠ” 20ëŒ€ íŠ¸ë Œë“œ í—Œí„° í†¤. ë§¤ìš° ìƒë™ê° ë„˜ì¹˜ëŠ” êµ¬ì–´ì²´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. (ì˜ˆ: "~ì˜ˆìš”!", "ì™„ì „ ëŒ€ë°•ì´ì£ ?", "ì´ê±° ì§€ê¸ˆ ë‚œë¦¬ ë‚¬ì–´ìš”!")

        [ë°ì´í„° ì»¨í…ìŠ¤íŠ¸]
        {self._serialize(context)}

        [ë¸Œë¦¬í•‘ í•„ìˆ˜ í¬í•¨ êµ¬ì¡°]
        1. **[ì „ëµì  í•©ì˜ ë¸Œë¦¬í•‘: ì œëª©]** (í•œê¸€ë¡œ ê°•ë ¬í•˜ê²Œ ì‘ì„±, 1.8rem ëŒ€ì œëª©ê¸‰ ê¶Œìœ„)
        2. **#### ğŸ“Š ë°ì´í„° íŒ©íŠ¸ì²´í¬**
           - ë„¤ì´ë²„ ê²€ìƒ‰ ì‹œê·¸ë„ | SNS í™•ì‚° ì†ë„ | ì»¤ë®¤ë‹ˆí‹° ì„¼í‹°ë©˜íŠ¸
        3. **#### ğŸ“˜ ì¥„ì˜ ë¦¬ìŠ¤í¬ & ê¸°íšŒ ë¶„ì„**: ê±°ì‹œ ì§€í‘œì™€ íŒ©íŠ¸ì— ê¸°ë°˜í•œ ëƒ‰ì •í•œ ê²½ì œ ì „ë§ (0.95rem ê³ ë°€ë„ ì •ë³´).
        4. **#### ğŸ”¥ ì¥í•ì˜ ë°”ì´ëŸ´ & íŠ¸ë Œë“œ í”½**: ì†Œì…œ ì—ë„ˆì§€ì™€ 'Vibe'ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ê°ê°ì  ì¸ì‚¬ì´íŠ¸.
        5. **#### ğŸ¤ ìµœì¢… ì „ëµì  í•©ì˜ ê²°ë¡ **: ë‘ ì‹œê°ì„ êµì°¨ ë¶„ì„í•˜ì—¬ ë„ì¶œëœ í•µì‹¬ ì‹¤í–‰ ì „ëµ.
        6. **#### ğŸš€ ì•¡ì…˜ í”Œëœ (Action Plan)**: ì‚¬ìš©ìê°€ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ êµ¬ì²´ì ì¸ ê¶Œê³ ì•ˆ.

        [ìŠ¤íƒ€ì¼ ê°€ì´ë“œ]
        - **ì½¤íŒ©íŠ¸ ìœ„ê³„**: ì •ë³´ ë°€ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ë¶ˆí•„ìš”í•œ ë¯¸ì‚¬ì—¬êµ¬ë¥¼ ë°°ì œí•˜ê³  í•µì‹¬ ìœ„ì£¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        - **ê³ ë°€ë„ ê°€ë…ì„±**: ìƒì„¸ ë‚´ìš©ì€ 0.95rem í¬ê¸°ì— ìµœì í™”ëœ ëª…í™•í•œ ë¬¸ì¥ ë¶€í˜¸ì™€ ë‹¨ë½ êµ¬ë¶„ì„ ì‚¬ìš©í•˜ì„¸ìš”.

        [í’ˆì§ˆ ê°œì„  í”¼ë“œë°± ë°˜ì˜]
        {feedback_context if feedback_context else "ì§€ì¹¨ì— ì¶©ì‹¤í•  ê²ƒ"}
        
        ì§€ê¸ˆ ë°”ë¡œ í•œêµ­ì–´ë¡œ ë¸Œë¦¬í•‘ì„ ì‹œì‘í•˜ì„¸ìš”.
        """

        # Using Gemma 3 12B via OllamaClient (with M4 acceleration & fallback)
        try:
            content = self.ollama.generate(
                prompt=prompt, 
                model="gemma3:12b", 
                temperature=0.4,
                max_tokens=2000
            )
        except Exception as e:
            print(f"âš ï¸ Gemma 3 failed, using Gemini Flash fallback: {e}")
            if self.model:
                response = self.model.generate_content(prompt)
                content = response.text
            else:
                return "Model generation failed."

        # Parse views for DB storage (Simplified regex/split pattern)
        jwem_view = content.split("ì¥„")[1].split("ì¥í•")[0] if "ì¥„" in content and "ì¥í•" in content else ""
        jfit_view = content.split("ì¥í•")[1].split("í•©ì˜")[0] if "ì¥í•" in content and "í•©ì˜" in content else ""

        # Save to consensus_briefings
        conn = self._get_db_conn()
        cur = conn.cursor()
        title = content.split('\n')[0].replace('#', '').replace('[', '').replace(']', '').strip()
        cur.execute("""
            INSERT INTO consensus_briefings (title, content, jwem_view, jfit_view, source_data)
            VALUES (%s, %s, %s, %s, %s)
            RETURNING id
        """, (title, content, jwem_view, jfit_view, self._serialize(context)))
        briefing_id = cur.fetchone()[0]
        conn.commit()
        cur.close()
        conn.close()

        return briefing_id

if __name__ == "__main__":
    gen = AnalysisGenerator()
    # Test generation
    try:
        rid = gen._save_report('System', 'Test', 'Init', 'Starting generation...', {})
        gen.generate_jwem_column()
        gen.generate_jfit_report()
        gen.generate_synthetic_spatial_insight()
        print("ğŸ‰ All analyses generated and saved.")
    except Exception as e:
        print(f"âŒ Error during generation: {e}")
